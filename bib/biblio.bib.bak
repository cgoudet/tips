% Encoding: UTF-8

@InProceedings{Ribeiro2016,
  author    = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  title     = {"Why Should I Trust You?": Explaining the Predictions of Any Classifier},
  booktitle = {Proceedings of the 22Nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2016},
  series    = {KDD '16},
  pages     = {1135--1144},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {2939778},
  doi       = {10.1145/2939672.2939778},
  file      = {:Ribeiro et al. - 2016 - Why Should I Trust You Explaining the Predicti.pdf:PDF},
  groups    = {explicabilite},
  isbn      = {978-1-4503-4232-2},
  keywords  = {black box classifier, explaining machine learning, interpretability, interpretable machine learning},
  location  = {San Francisco, California, USA},
  numpages  = {10},
  url       = {http://doi.acm.org/10.1145/2939672.2939778},
}

@InProceedings{Sathe2017,
  author    = {Sathe, Saket and Aggarwal, Charu C.},
  title     = {Similarity Forests},
  booktitle = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  year      = {2017},
  series    = {KDD '17},
  pages     = {395--403},
  address   = {New York, NY, USA},
  publisher = {ACM},
  acmid     = {3098046},
  doi       = {10.1145/3097983.3098046},
  file      = {:Sathe2017 - Similarity Forests.pdf:PDF},
  isbn      = {978-1-4503-4887-4},
  keywords  = {classification, data mining, random forests},
  location  = {Halifax, NS, Canada},
  numpages  = {9},
  url       = {http://doi.acm.org/10.1145/3097983.3098046},
}

@Article{Ishwaran2008,
  author       = {Hemant Ishwaran and Udaya B. Kogalur and Eugene H. Blackstone and Michael S. Lauer},
  title        = {Random survival forests},
  abstract     = {We introduce random survival forests, a random forests method for the analysis of right-censored survival data. New survival splitting rules for growing survival trees are introduced, as is a new missing data algorithm for imputing missing data. A conservation-of-events principle for survival forests is introduced and used to define ensemble mortality, a simple interpretable measure of mortality that can be used as a predicted outcome. Several illustrative examples are given, including a case study of the prognostic implications of body mass for individuals with coronary artery disease. Computations for all examples were implemented using the freely available R-software package, randomSurvivalForest.},
  date         = {2008-11-11},
  doi          = {10.1214/08-AOAS169},
  eprint       = {http://arxiv.org/abs/0811.1645v1},
  eprintclass  = {stat.AP},
  eprinttype   = {arXiv},
  file         = {:Ishwaran2008 - Random Survival Forests.pdf:PDF},
  journaltitle = {Annals of Applied Statistics 2008, Vol. 2, No. 3, 841-860},
  keywords     = {stat.AP},
  url          = {http://arxiv.org/pdf/0811.1645v1},
}

@Article{Barriere2019,
  author      = {Valentin Barriere and Amaury Fouret},
  title       = {May I Check Again? -- A simple but efficient way to generate and use contextual dictionaries for Named Entity Recognition. Application to French Legal Texts},
  journal     = {Arxiv},
  year        = {2019},
  abstract    = {In this paper we present a new method to learn a model robust to typos for a Named Entity Recognition task. Our improvement over existing methods helps the model to take into account the context of the sentence inside a court decision in order to recognize an entity with a typo. We used state-of-the-art models and enriched the last layer of the neural network with high-level information linked with the potential of the word to be a certain type of entity. More precisely, we utilized the similarities between the word and the potential entity candidates in the tagged sentence context. The experiments on a dataset of French court decisions show a reduction of the relative F1-score error of 32%, upgrading the score obtained with the most competitive fine-tuned state-of-the-art system from 94.85% to 96.52%.},
  date        = {2019-09-08},
  eprint      = {http://arxiv.org/abs/1909.03453v1},
  eprintclass = {cs.CL},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1909.03453v1:PDF;:Barriere2019 - May I Check Again_ a Simple but Efficient Way to Generate and Use Contextual Dictionaries for Named Entity Recognition. Application to French Legal Texts.pdf:PDF},
  keywords    = {cs.CL},
  url         = {http://arxiv.org/pdf/1909.03453v1},
}

@Article{Wang2015,
  author      = {Zhiguang Wang and Tim Oates},
  title       = {{Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks}},
  journal     = {Arxiv},
  year        = {2015},
  abstract    = {We propose an off-line approach to explicitly encode temporal patterns spatially as different types of images, namely, Gramian Angular Fields and Markov Transition Fields. This enables the use of techniques from computer vision for feature learning and classification. We used Tiled Convolutional Neural Networks to learn high-level features from individual GAF, MTF, and GAF-MTF images on 12 benchmark time series datasets and two real spatial-temporal trajectory datasets. The classification results of our approach are competitive with state-of-the-art approaches on both types of data. An analysis of the features and weights learned by the CNNs explains why the approach works.},
  date        = {2015-09-24},
  eprint      = {http://arxiv.org/abs/1509.07481v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Wang2015 - Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks.pdf:PDF;:Wang2015 - Spatially Encoding Temporal Correlations to Classify Temporal Data Using Convolutional Neural Networks.pdf:PDF},
  keywords    = {cs.LG},
  url         = {http://arxiv.org/pdf/1509.07481v1},
}

@Article{Maaten2008,
  author  = {Maaten, Laurens van der and Hinton, Geoffrey},
  title   = {Visualizing data using t-SNE},
  journal = {Journal of machine learning research},
  year    = {2008},
  volume  = {9},
  number  = {Nov},
  pages   = {2579--2605},
  file    = {:http\://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf:PDF},
  groups  = {unsupervised},
  url     = {http://www.jmlr.org/papers/volume9/vandermaaten08a/vandermaaten08a.pdf},
}

@Article{Rousseeuw1987,
  author    = {Rousseeuw, Peter J},
  title     = {Silhouettes: a graphical aid to the interpretation and validation of cluster analysis},
  journal   = {Journal of computational and applied mathematics},
  year      = {1987},
  volume    = {20},
  pages     = {53--65},
  file      = {:Rousseeuw1987 - Silhouettes_ a Graphical Aid to the Interpretation and Validation of Cluster Analysis.pdf:PDF},
  groups    = {unsupervised},
  publisher = {Elsevier},
  url       = {https://www.researchgate.net/publication/222451107_Rousseeuw_PJ_Silhouettes_A_Graphical_Aid_to_the_Interpretation_and_Validation_of_Cluster_Analysis_Comput_Appl_Math_20_53-65/link/59e1d354458515393d57bbdf/download},
}

@Article{Lundberg2017,
  author      = {Scott Lundberg and Su-In Lee},
  title       = {A Unified Approach to Interpreting Model Predictions},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  date        = {2017-05-22},
  eprint      = {http://arxiv.org/abs/1705.07874v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:Lundberg2017 - A Unified Approach to Interpreting Model Predictions.pdf:PDF},
  keywords    = {cs.AI, cs.LG, stat.ML},
  url         = {http://arxiv.org/pdf/1705.07874v2},
}

@Article{Karimi2018,
  author      = {Mostafa Karimi and Di Wu and Zhangyang Wang and Yang Shen},
  title       = {DeepAffinity: Interpretable Deep Learning of Compound-Protein Affinity through Unified Recurrent and Convolutional Neural Networks},
  journal     = {Arxiv},
  year        = {2018},
  abstract    = {Motivation: Drug discovery demands rapid quantification of compound-protein interaction (CPI). However, there is a lack of methods that can predict compound-protein affinity from sequences alone with high applicability, accuracy, and interpretability. Results: We present a seamless integration of domain knowledges and learning-based approaches. Under novel representations of structurally-annotated protein sequences, a semi-supervised deep learning model that unifies recurrent and convolutional neural networks has been proposed to exploit both unlabeled and labeled data, for jointly encoding molecular representations and predicting affinities. Our representations and models outperform conventional options in achieving relative error in IC$_{50}$ within 5-fold for test cases and 20-fold for protein classes not included for training. Performances for new protein classes with few labeled data are further improved by transfer learning. Furthermore, separate and joint attention mechanisms are developed and embedded to our model to add to its interpretability, as illustrated in case studies for predicting and explaining selective drug-target interactions. Lastly, alternative representations using protein sequences or compound graphs and a unified RNN/GCNN-CNN model using graph CNN (GCNN) are also explored to reveal algorithmic challenges ahead. Availability: Data and source codes are available at https://github.com/Shen-Lab/DeepAffinity Supplementary Information: Supplementary data are available at http://shen-lab.github.io/deep-affinity-bioinf18-supp-rev.pdf},
  date        = {2018-06-20},
  eprint      = {http://arxiv.org/abs/1806.07537v2},
  eprintclass = {q-bio.BM},
  eprinttype  = {arXiv},
  file        = {:Karimi2018 - DeepAffinity_ Interpretable Deep Learning of Compound Protein Affinity through Unified Recurrent and Convolutional Neural Networks.pdf:PDF},
  keywords    = {q-bio.BM, cs.LG, stat.ML},
  url         = {http://arxiv.org/pdf/1806.07537v2},
}

@Article{Leng2010,
  author      = {Chenlei Leng and Minh Ngoc Tran and David Nott},
  title       = {Bayesian Adaptive Lasso},
  journal     = {Arxiv},
  year        = {2010},
  abstract    = {We propose the Bayesian adaptive Lasso (BaLasso) for variable selection and coefficient estimation in linear regression. The BaLasso is adaptive to the signal level by adopting different shrinkage for different coefficients. Furthermore, we provide a model selection machinery for the BaLasso by assessing the posterior conditional mode estimates, motivated by the hierarchical Bayesian interpretation of the Lasso. Our formulation also permits prediction using a model averaging strategy. We discuss other variants of this new approach and provide a unified framework for variable selection using flexible penalties. Empirical evidence of the attractiveness of the method is demonstrated via extensive simulation studies and data analysis.},
  date        = {2010-09-13},
  eprint      = {http://arxiv.org/abs/1009.2300v1},
  eprintclass = {stat.ME},
  eprinttype  = {arXiv},
  file        = {:Leng2010 - Bayesian Adaptive Lasso.pdf:PDF;:Leng2010 - Bayesian Adaptive Lasso.pdf:PDF},
  keywords    = {stat.ME, stat.CO},
  url         = {http://arxiv.org/pdf/1009.2300v1},
}

@Article{Takeishi2019,
  author      = {Naoya Takeishi},
  title       = {Shapley Values of Reconstruction Errors of PCA for Explaining Anomaly Detection},
  journal     = {Arxiv},
  year        = {2019},
  abstract    = {We present a method to compute the Shapley values of reconstruction errors of principal component analysis (PCA), which is particularly useful in explaining the results of anomaly detection based on PCA. Because features are usually correlated when PCA-based anomaly detection is applied, care must be taken in computing a value function for the Shapley values. We utilize the probabilistic view of PCA, particularly its conditional distribution, to exactly compute a value function for the Shapely values. We also present numerical examples, which imply that the Shapley values are advantageous for explaining detected anomalies than raw reconstruction errors of each feature.},
  date        = {2019-09-08},
  eprint      = {http://arxiv.org/abs/1909.03495v1},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1909.03495v1:PDF},
  keywords    = {stat.ML, cs.LG},
}

@Article{Li2019,
  author      = {Yadong Li and Xin Cui},
  title       = {{Shapley Interpretation and Activation in Neural Networks}},
  journal     = {Arxiv},
  year        = {2019},
  abstract    = {We propose a novel Shapley value approach to help address neural networks' interpretability and "vanishing gradient" problems. Our method is based on an accurate analytical approximation to the Shapley value of a neuron with ReLU activation. This analytical approximation admits a linear propagation of relevance across neural network layers, resulting in a simple, fast and sensible interpretation of neural networks' decision making process. We then derived a globally continuous and non-vanishing Shapley gradient, which can replace the conventional gradient in training neural network layers with ReLU activation, and leading to better training performance. We further derived a Shapley Activation (SA) function, which is a close approximation to ReLU but features the Shapley gradient. The SA is easy to implement in existing machine learning frameworks. Numerical tests show that SA consistently outperforms ReLU in training convergence, accuracy and stability.},
  date        = {2019-09-13},
  eprint      = {http://arxiv.org/abs/1909.06143v2},
  eprintclass = {stat.ML},
  eprinttype  = {arXiv},
  file        = {:http\://arxiv.org/pdf/1909.06143v2:PDF;:http\://arxiv.org/pdf/1909.06143v2:},
  keywords    = {stat.ML, cs.LG, cs.NE},
  url         = {http://arxiv.org/pdf/1909.06143v2},
}

@Article{Reichheld2001,
  author  = {Reichheld, Fred},
  title   = {Prescription for cutting costs},
  journal = {Bain \& Company},
  year    = {2001},
  file    = {:Reichheld2001.pdf:PDF},
  groups  = {marketing},
  url     = {https://www.bain.com/contentassets/2598a2341fed40eba41954ee442ead22/bb_prescription_cutting_costs.pdf},
}

@Article{Schmarzo2016,
  author  = {Schmarzo, Bill},
  title   = {{What Defines Your “Most Valuable” Customers?}},
  journal = {Linkedin},
  year    = {2016},
  file    = {:Schmarzo2016.pdf:PDF},
  groups  = {marketing},
  url     = {https://www.linkedin.com/pulse/what-defines-your-most-valuable-customers-bill-schmarzo/},
}

@Article{Lagacherie2013,
  author  = {Matthieu Lagacherie},
  title   = {{Apprentissage distribué avec Spark}},
  journal = {Octo Talks},
  year    = {2013},
  file    = {:Lagacherie2013 - Apprentissage Distribué Avec Spark.html:URL},
  url     = {https://blog.octo.com/apprentissage-distribue-avec-spark/},
}

@Article{Massiot2019,
  author  = {Massiot, Aurélien},
  title   = {{Machine Learning – 7 astuces pour scaler Python sur de grands datasets }},
  journal = {Octo Talks},
  year    = {2019},
  file    = {:Massiot2019.pdf:PDF},
  groups  = {python},
  url     = {https://blog.octo.com/machine-learning-7-astuces-pour-scaler-python-sur-de-grands-datasets/},
}

@Article{kruegger201902,
  author  = {Alex Kruegger},
  title   = {How to “farm” Kaggle in the right way},
  journal = {Medium},
  year    = {2018},
  file    = {:kruegger201902.pdf:PDF},
  groups  = {DS_process},
  url     = {https://towardsdatascience.com/how-to-farm-kaggle-in-the-right-way-b27f781b78da},
}

@TechReport{Fliche2018,
  author      = {Fliche, Olivier and Yang Su},
  title       = {Intelligence artificielle : enjeux pour le secteur financier},
  institution = {ACPR},
  year        = {2018},
  comment     = {La fiabilité des algorithmes passe ensuite par la vérification que l’usage des données est approprié au regard des objectifs fixés et qu’il n’induit pas de biais involontaires. Plusieurs méthodes sont envisagées par les acteurs financiers à cet égard :
 Recours à des experts pour valider la pertinence des variables utilisées, éliminer celles qui sont inutiles24 ou sources de biais potentiels ;
 Emploi d’un processus parallèle plus sûr et plus traditionnel sur une partie des données tests ;
 Utilisation d’un jeu de données étalon sur les algorithmes pour contrôler régulièrement à la fois la pertinence et l’aspect non-discriminatoire des algorithmes ;
 Développement d’outils qui évalueraient la dérive conceptuelle pour maîtriser ce risque spécifique de l’apprentissage automatique

Le professionnel doit donc se mettre en mesure d’expliquer :
 De façon générale, quels sont les mécanismes et les critères suivis par l’algorithme au cours de son processus d’analyse ;
 Pour une action donnée (une décision prise, un conseil fourni) les critères objectifs et les éléments discriminants qui ont poussé l’algorithme, dans le cas étudié, à effectuer une action ou proposer une solution plutôt qu’une autre.},
  file        = {:Fliche2018 - Intelligence Artificielle _ Enjeux Pour Le Secteur Financier.pdf:PDF},
  groups      = {reglementation},
  url         = {https://acpr.banque-france.fr/sites/default/files/medias/documents/2018_12_20_intelligence_artificielle_fr_0.pdf},
}

@Article{Geurts2006,
  author    = {Geurts, Pierre and Ernst, Damien and Wehenkel, Louis},
  title     = {Extremely randomized trees},
  journal   = {Machine learning},
  year      = {2006},
  volume    = {63},
  number    = {1},
  pages     = {3--42},
  file      = {:Geurts2006 - Extremely Randomized Trees.pdf:PDF},
  groups    = {supervised},
  publisher = {Springer},
  url       = {https://link.springer.com/content/pdf/10.1007/s10994-006-6226-1.pdf},
}

@Article{Ensign2017,
  author      = {Danielle Ensign and Sorelle A. Friedler and Scott Neville and Carlos Scheidegger and Suresh Venkatasubramanian},
  title       = {Runaway Feedback Loops in Predictive Policing},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {Predictive policing systems are increasingly used to determine how to allocate police across a city in order to best prevent crime. Discovered crime data (e.g., arrest counts) are used to help update the model, and the process is repeated. Such systems have been empirically shown to be susceptible to runaway feedback loops, where police are repeatedly sent back to the same neighborhoods regardless of the true crime rate. In response, we develop a mathematical model of predictive policing that proves why this feedback loop occurs, show empirically that this model exhibits such problems, and demonstrate how to change the inputs to a predictive policing system (in a black-box manner) so the runaway feedback loop does not occur, allowing the true crime rate to be learned. Our results are quantitative: we can establish a link (in our model) between the degree to which runaway feedback causes problems and the disparity in crime rates between areas. Moreover, we can also demonstrate the way in which \emph{reported} incidents of crime (those reported by residents) and \emph{discovered} incidents of crime (i.e. those directly observed by police officers dispatched as a result of the predictive policing algorithm) interact: in brief, while reported incidents can attenuate the degree of runaway feedback, they cannot entirely remove it without the interventions we suggest.},
  date        = {2017-06-29},
  eprint      = {http://arxiv.org/abs/1706.09847v3},
  eprintclass = {cs.CY},
  eprinttype  = {arXiv},
  file        = {:Ensign2017 - Runaway Feedback Loops in Predictive Policing:},
  groups      = {reglementation},
  keywords    = {cs.CY, stat.ML},
  url         = {http://arxiv.org/pdf/1706.09847v3},
}

@Article{Feng2017,
  author      = {Ji Feng and Zhi-Hua Zhou},
  title       = {AutoEncoder by Forest},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {Auto-encoding is an important task which is typically realized by deep neural networks (DNNs) such as convolutional neural networks (CNN). In this paper, we propose EncoderForest (abbrv. eForest), the first tree ensemble based auto-encoder. We present a procedure for enabling forests to do backward reconstruction by utilizing the equivalent classes defined by decision paths of the trees, and demonstrate its usage in both supervised and unsupervised setting. Experiments show that, compared with DNN autoencoders, eForest is able to obtain lower reconstruction error with fast training speed, while the model itself is reusable and damage-tolerable.},
  date        = {2017-09-26},
  eprint      = {http://arxiv.org/abs/1709.09018v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Feng2017 - AutoEncoder by Forest:},
  groups      = {unsupervised},
  keywords    = {cs.LG, stat.ML},
  url         = {http://arxiv.org/pdf/1709.09018v1},
}

@TechReport{AIHLEGPolicyandInvestmentRecommendationspdf,
  author      = {{High-Level Expert Group on AI (AI HLEG)}},
  title       = {{Policy and investment recommendations for trustworthy Artificial Intelligence}},
  institution = {European Commission},
  year        = {2018},
  file        = {:AIHLEGPolicyandInvestmentRecommendationspdf.pdf:PDF},
  groups      = {reglementation},
  url         = {https://ec.europa.eu/digital-single-market/en/news/policy-and-investment-recommendations-trustworthy-artificial-intelligence},
}

@TechReport{AIHLEG_EthicsGuidelinesforTrustworthyAI-ENpdf,
  author      = {{High-Level Expert Group on Artificial Intelligence (AI HLEG)}},
  title       = {{Ethics Guidelines for Trustworthy AI}},
  institution = {European Commission},
  year        = {2018},
  groups      = {reglementation},
  url         = {https://ec.europa.eu/futurium/en/ai-alliance-consultation/guidelines#Top},
}

@Article{Lundberg2017a,
  author      = {Scott Lundberg and Su-In Lee},
  title       = {A Unified Approach to Interpreting Model Predictions},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.},
  date        = {2017-05-22},
  eprint      = {http://arxiv.org/abs/1705.07874v2},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:Lundberg2017a - A Unified Approach to Interpreting Model Predictions.pdf:PDF},
  groups      = {explicabilite},
  keywords    = {cs.AI, cs.LG, stat.ML},
  url         = {http://arxiv.org/pdf/1705.07874v2},
}

@TechReport{CERNA201706,
  author      = {CERNA},
  title       = {{Éthique de la recherche en apprentissage machine}},
  institution = {CERNA},
  year        = {2017},
  file        = {:CERNA201706 - Éthique De La Recherche En Apprentissage Machine.pdf:PDF},
  url         = {http://cerna-ethics-allistene.org/digitalAssets/53/53991_cerna___thique_apprentissage.pdf},
}

@TechReport{EurComIA2019,
  author      = {Commissioner for Human Rights},
  title       = {Unboxing Artificial Intelligence: 10 steps to protect Human Rights},
  institution = {Council of Europe Commissioner for Human Rights},
  year        = {2019},
  file        = {:-.pdf:PDF},
  url         = {https://rm.coe.int/unboxing-artificial-intelligence-10-steps-to-protect-human-rights-reco/1680946e64},
}

@Article{Holzinger2017,
  author      = {Andreas Holzinger and Chris Biemann and Constantinos S. Pattichis and Douglas B. Kell},
  title       = {What do we need to build explainable AI systems for the medical domain?},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {Artificial intelligence (AI) generally and machine learning (ML) specifically demonstrate impressive practical success in many different application domains, e.g. in autonomous driving, speech recognition, or recommender systems. Deep learning approaches, trained on extremely large data sets or using reinforcement learning methods have even exceeded human performance in visual tasks, particularly on playing games such as Atari, or mastering the game of Go. Even in the medical domain there are remarkable results. The central problem of such models is that they are regarded as black-box models and even if we understand the underlying mathematical principles, they lack an explicit declarative knowledge representation, hence have difficulty in generating the underlying explanatory structures. This calls for systems enabling to make decisions transparent, understandable and explainable. A huge motivation for our approach are rising legal and privacy aspects. The new European General Data Protection Regulation entering into force on May 25th 2018, will make black-box approaches difficult to use in business. This does not imply a ban on automatic learning approaches or an obligation to explain everything all the time, however, there must be a possibility to make the results re-traceable on demand. In this paper we outline some of our research topics in the context of the relatively new area of explainable-AI with a focus on the application in medicine, which is a very special domain. This is due to the fact that medical professionals are working mostly with distributed heterogeneous and complex sources of data. In this paper we concentrate on three sources: images, *omics data and text. We argue that research in explainable-AI would generally help to facilitate the implementation of AI/ML in the medical domain, and specifically help to facilitate transparency and trust.},
  date        = {2017-12-28},
  eprint      = {http://arxiv.org/abs/1712.09923v1},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:Holzinger2017 - What Do We Need to Build Explainable AI Systems for the Medical Domain_:},
  keywords    = {cs.AI, stat.ML},
  url         = {http://arxiv.org/pdf/1712.09923v1},
}

@Article{Doran2017,
  author      = {Derek Doran and Sarah Schulz and Tarek R. Besold},
  title       = {What Does Explainable AI Really Mean? A New Conceptualization of Perspectives},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {We characterize three notions of explainable AI that cut across research fields: opaque systems that offer no insight into its algo- rithmic mechanisms; interpretable systems where users can mathemat- ically analyze its algorithmic mechanisms; and comprehensible systems that emit symbols enabling user-driven explanations of how a conclusion is reached. The paper is motivated by a corpus analysis of NIPS, ACL, COGSCI, and ICCV/ECCV paper titles showing differences in how work on explainable AI is positioned in various fields. We close by introducing a fourth notion: truly explainable systems, where automated reasoning is central to output crafted explanations without requiring human post processing as final step of the generative process.},
  date        = {2017-10-02},
  eprint      = {http://arxiv.org/abs/1710.00794v1},
  eprintclass = {cs.AI},
  eprinttype  = {arXiv},
  file        = {:Doran2017 - What Does Explainable AI Really Mean_ a New Conceptualization of Perspectives:},
  keywords    = {cs.AI},
  url         = {http://arxiv.org/pdf/1710.00794v1},
}

@Article{Bennetot2019,
  author      = {Adrien Bennetot and Jean-Luc Laurent and Raja Chatila and Natalia Díaz-Rodríguez},
  title       = {Highlighting Bias with Explainable Neural-Symbolic Visual Reasoning},
  journal     = {Arxiv},
  year        = {2017},
  abstract    = {Many high-performance models suffer from a lack of interpretability. There has been an increasing influx of work on explainable artificial intelligence (XAI) in order to disentangle what is meant and expected by XAI. Nevertheless, there is no general consensus on how to produce and judge explanations. In this paper, we discuss why techniques integrating connectionist and symbolic paradigms are the most efficient solutions to produce explanations for non-technical users and we propose a reasoning model, based on definitions by Doran et al. [2017] (arXiv:1710.00794) to explain a neural network's decision. We use this explanation in order to correct bias in the network's decision rationale. We accompany this model with an example of its potential use, based on the image captioning method in Burns et al. [2018] (arXiv:1803.09797).},
  date        = {2019-09-19},
  eprint      = {http://arxiv.org/abs/1909.09065v1},
  eprintclass = {cs.LG},
  eprinttype  = {arXiv},
  file        = {:Bennetot2019 - Highlighting Bias with Explainable Neural Symbolic Visual Reasoning:},
  keywords    = {cs.LG, cs.AI},
  url         = {http://arxiv.org/pdf/1909.09065v1},
}

@Article{Hendricks2018,
  author       = {Lisa Anne Hendricks and Ronghang Hu and Trevor Darrell and Zeynep Akata},
  title        = {Grounding Visual Explanations},
  journal      = {Arxiv},
  year         = {2018},
  abstract     = {Existing visual explanation generating agents learn to fluently justify a class prediction. However, they may mention visual attributes which reflect a strong class prior, although the evidence may not actually be in the image. This is particularly concerning as ultimately such agents fail in building trust with human users. To overcome this limitation, we propose a phrase-critic model to refine generated candidate explanations augmented with flipped phrases which we use as negative examples while training. At inference time, our phrase-critic model takes an image and a candidate explanation as input and outputs a score indicating how well the candidate explanation is grounded in the image. Our explainable AI agent is capable of providing counter arguments for an alternative prediction, i.e. counterfactuals, along with explanations that justify the correct classification decisions. Our model improves the textual explanation quality of fine-grained classification decisions on the CUB dataset by mentioning phrases that are grounded in the image. Moreover, on the FOIL tasks, our agent detects when there is a mistake in the sentence, grounds the incorrect phrase and corrects it significantly better than other models.},
  date         = {2018-07-25},
  eprint       = {http://arxiv.org/abs/1807.09685v2},
  eprintclass  = {cs.CV},
  eprinttype   = {arXiv},
  file         = {:Hendricks2018 - Grounding Visual Explanations:},
  journaltitle = {European Conference on Computer Vision (ECCV), 2018},
  keywords     = {cs.CV},
  url          = {http://arxiv.org/pdf/1807.09685v2},
}

@Article{Goyal2019,
  author        = {Yash Goyal and Ziyan Wu and Jan Ernst and Dhruv Batra and Devi Parikh and Stefan Lee},
  title         = {Counterfactual Visual Explanations},
  journal       = {Arxiv},
  year          = {2019},
  __markedentry = {[christophe_goudet:]},
  abstract      = {In this work, we develop a technique to produce counterfactual visual explanations. Given a 'query' image $I$ for which a vision system predicts class $c$, a counterfactual visual explanation identifies how $I$ could change such that the system would output a different specified class $c'$. To do this, we select a 'distractor' image $I'$ that the system predicts as class $c'$ and identify spatial regions in $I$ and $I'$ such that replacing the identified region in $I$ with the identified region in $I'$ would push the system towards classifying $I$ as $c'$. We apply our approach to multiple image classification datasets generating qualitative results showcasing the interpretability and discriminativeness of our counterfactual explanations. To explore the effectiveness of our explanations in teaching humans, we present machine teaching experiments for the task of fine-grained bird classification. We find that users trained to distinguish bird species fare better when given access to counterfactual explanations in addition to training examples.},
  date          = {2019-04-16},
  eprint        = {http://arxiv.org/abs/1904.07451v2},
  eprintclass   = {cs.LG},
  eprinttype    = {arXiv},
  file          = {:Goyal2019 - Counterfactual Visual Explanations:},
  keywords      = {cs.LG, cs.AI, cs.CV, stat.ML},
  url           = {http://arxiv.org/pdf/1904.07451v2},
}

@Article{Alcorn2018,
  author        = {Michael A. Alcorn and Qi Li and Zhitao Gong and Chengfei Wang and Long Mai and Wei-Shinn Ku and Anh Nguyen},
  title         = {Strike (with) a Pose: Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects},
  journal       = {Arxiv},
  year          = {2},
  __markedentry = {[christophe_goudet:6]},
  abstract      = {Despite excellent performance on stationary test sets, deep neural networks (DNNs) can fail to generalize to out-of-distribution (OoD) inputs, including natural, non-adversarial ones, which are common in real-world settings. In this paper, we present a framework for discovering DNN failures that harnesses 3D renderers and 3D models. That is, we estimate the parameters of a 3D renderer that cause a target DNN to misbehave in response to the rendered image. Using our framework and a self-assembled dataset of 3D objects, we investigate the vulnerability of DNNs to OoD poses of well-known objects in ImageNet. For objects that are readily recognized by DNNs in their canonical poses, DNNs incorrectly classify 97% of their pose space. In addition, DNNs are highly sensitive to slight pose perturbations. Importantly, adversarial poses transfer across models and datasets. We find that 99.9% and 99.4% of the poses misclassified by Inception-v3 also transfer to the AlexNet and ResNet-50 image classifiers trained on the same ImageNet dataset, respectively, and 75.5% transfer to the YOLOv3 object detector trained on MS COCO.},
  date          = {2018-11-28},
  eprint        = {http://arxiv.org/abs/1811.11553v3},
  eprintclass   = {cs.CV},
  eprinttype    = {arXiv},
  file          = {:Alcorn2018 - Strike (with) a Pose_ Neural Networks Are Easily Fooled by Strange Poses of Familiar Objects:},
  keywords      = {cs.CV, cs.LG},
  url           = {http://arxiv.org/pdf/1811.11553v3},
}

@Article{strobl07,
  author  = {Carolin Strobl and Anne-Laure Boulesteix and Achim Zeileis and Torsten Hothorn4},
  title   = {Bias in random forest variable importance measures: Illustrations, sources and a solution},
  journal = {BMC Bioinformatics},
  year    = {2007},
  file    = {:strobl07 - Bias in Random Forest Variable Importance Measures_ Illustrations, Sources and a Solution:},
  groups  = {DS_process},
  url     = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-25},
}

@Article{Srivastava_1510,
  author  = {Tavish Srivastava},
  title   = {13 Tips to make you awesome in Data Science / Analytics Jobs},
  journal = {Analytics Vidhya},
  year    = {2015},
  file    = {:C\:/Users/christophe_goudet/ownCloud/Veille/Reread/13 Tips to make you awesome in Data Science.pdf:PDF},
  groups  = {DS_process},
  url     = {https://www.analyticsvidhya.com/blog/2015/10/tips-tricks-awesom-data-science-jobs/},
}

@Comment{jabref-meta: databaseType:bibtex;}

@Comment{jabref-meta: grouping:
0 AllEntriesGroup:;
1 StaticGroup:explicabilite\;0\;1\;\;\;\;;
1 StaticGroup:unsupervised\;0\;1\;\;\;\;;
1 StaticGroup:marketing\;0\;1\;\;\;\;;
1 StaticGroup:python\;0\;1\;\;\;\;;
1 StaticGroup:DS_process\;0\;1\;\;\;\;;
1 StaticGroup:reglementation\;0\;1\;\;\;\;;
1 StaticGroup:supervised\;0\;1\;\;\;\;;
}
